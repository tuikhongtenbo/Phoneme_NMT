# LSTM + Luong Attention configuration

model:
  name: "lstm_luong"
  embed_dim: 512
  hidden_dim: 512
  num_layers: 2
  dropout: 0.1
  attention_type: "general"  # Luong attention types: general, dot, concat

training:
  batch_size: 32
  num_epochs: 50
  learning_rate: 0.001
  optimizer: "adam"
  scheduler: "cosine"
  clip_grad_norm: 5.0
  warmup_steps: null
  eval_every: 1000
  save_every: 5000

data:
  # Special token IDs
  sos_id: 1
  eos_id: 2
  pad_id: 0
  unk_id: 3
  
  # Target level: 'word' or 'phoneme'
  target_level: "phoneme"
  
  # Direct paths to training data files
  train_src: "dataset/vocabs/clean/train_end.en"
  train_tgt: "dataset/vocabs/clean/train_end.vi"
  
  # Direct paths to dev data files
  dev_src: "dataset/vocabs/clean/dev_end.en"
  dev_tgt: "dataset/vocabs/clean/dev_end.vi"
  
  # Direct paths to test data files
  test_src: "dataset/vocabs/clean/test_end.en"
  test_tgt: "dataset/vocabs/clean/test_end.vi"
  
  # Direct paths to vocabulary JSON files (for phoneme-level)
  vocab_json_train: "dataset/vocabs/full_vocab_ipa.json"
  vocab_json_dev: "dataset/vocabs/full_vocab_ipa.json"
  vocab_json_test: "dataset/vocabs/full_vocab_ipa.json"
  
  # Vocabulary settings
  min_count: 3
  max_seq_len: 100

device: "cuda"
seed: 42