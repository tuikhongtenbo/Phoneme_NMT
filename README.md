# English-Vietnamese Neural Machine Translation

A research project implementing baseline Neural Machine Translation (NMT) models for English-Vietnamese translation, supporting both **word-level** and **phoneme-level** processing.

## Overview

This project provides comprehensive implementations of baseline NMT architectures for English-Vietnamese machine translation research:

- **LSTM + Bahdanau Attention**: Sequence-to-sequence model with additive attention mechanism
- **LSTM + Luong Attention**: Sequence-to-sequence model with multiplicative attention (general, dot, concat variants)
- **Transformer**: Attention-based architecture following "Attention is All You Need" (Vaswani et al., 2017)

All models support:
- Training and inference pipelines
- Comprehensive evaluation with BLEU, ROUGE, and METEOR metrics
- Both word-level and phoneme-level processing
- Autoregressive decoding with state caching
- Flexible configuration via YAML files and command-line arguments

## Project Structure

```
Phoneme_NMT/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ models/                   # Model implementations
â”‚   â”‚   â”œâ”€â”€ base_model.py         # Abstract base model class
â”‚   â”‚   â”œâ”€â”€ attention/            # Attention mechanisms (for LSTM models)
â”‚   â”‚   â”‚   â”œâ”€â”€ bahdanau.py       # Bahdanau Attention
â”‚   â”‚   â”‚   â””â”€â”€ luong.py          # Luong Attention
â”‚   â”‚   â”œâ”€â”€ lstm/                 # LSTM-based models
â”‚   â”‚   â”‚   â”œâ”€â”€ encoder.py        # LSTM Encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ lstm_bahdanau.py  # LSTM + Bahdanau Attention
â”‚   â”‚   â”‚   â””â”€â”€ lstm_luong.py     # LSTM + Luong Attention
â”‚   â”‚   â””â”€â”€ transformer/          # Transformer (modular architecture)
â”‚   â”‚       â”œâ”€â”€ transformer.py    # Main Transformer model
â”‚   â”‚       â”œâ”€â”€ encoder.py         # Transformer Encoder
â”‚   â”‚       â”œâ”€â”€ decoder.py         # Transformer Decoder
â”‚   â”‚       â”œâ”€â”€ blocks/           # Encoder/Decoder layers
â”‚   â”‚       â”‚   â”œâ”€â”€ encoder_layer.py
â”‚   â”‚       â”‚   â””â”€â”€ decoder_layer.py
â”‚   â”‚       â”œâ”€â”€ layers/           # Core attention & feed-forward layers
â”‚   â”‚       â”‚   â”œâ”€â”€ multi_head_attention.py
â”‚   â”‚       â”‚   â”œâ”€â”€ scale_dot_product_attention.py
â”‚   â”‚       â”‚   â””â”€â”€ position_wise_feed_forward.py
â”‚   â”‚       â””â”€â”€ embedding/        # Embedding components
â”‚   â”‚           â”œâ”€â”€ positional_encoding.py
â”‚   â”‚           â”œâ”€â”€ token_embeddings.py
â”‚   â”‚           â””â”€â”€ transformer_embedding.py
â”‚   â”œâ”€â”€ training/                 # Training infrastructure
â”‚   â”‚   â””â”€â”€ trainer.py            # Main training class
â”‚   â”œâ”€â”€ evaluation/               # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ evaluator.py          # Main evaluation class
â”‚   â”‚   â”œâ”€â”€ bleu.py               # BLEU score implementation
â”‚   â”‚   â”œâ”€â”€ rouge.py              # ROUGE score implementation
â”‚   â”‚   â””â”€â”€ meteor.py             # METEOR score implementation
â”‚   â”œâ”€â”€ data/                     # Data processing
â”‚   â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ preprocessing.py      # Data preprocessing
â”‚   â”‚   â””â”€â”€ vocabs/               # Vocabulary classes
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â””â”€â”€ logger.py             # Logging utilities
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ config.py                 # Config management (Pydantic-based)
â”‚   â”œâ”€â”€ lstm_bahdanau.yaml        # LSTM + Bahdanau configuration
â”‚   â”œâ”€â”€ lstm_luong.yaml           # LSTM + Luong configuration
â”‚   â””â”€â”€ transformer.yaml          # Transformer configuration
â”œâ”€â”€ dataset/                      # Raw data directory
â”‚   â””â”€â”€ vocabs/                   # Vocabulary files
â”œâ”€â”€ checkpoints/                  # Model checkpoints
â”œâ”€â”€ logs/                         # Training logs
â”œâ”€â”€ results/                      # Experiment results
â”œâ”€â”€ main.py                       # Main entry point
â””â”€â”€ test_*.py                     # Test scripts
```

## Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended for training)

### Setup

1. **Clone the repository**

```bash
git clone https://github.com/tuikhongtenbo/Phoneme_NMT.git
cd Phoneme_NMT
```

2. **Create a virtual environment** 

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/macOS
python -m venv venv
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

## Usage

### Training

#### Basic Training

Train a model using a configuration file:

```bash
# Train Transformer model
python main.py --config configs/transformer.yaml

# Train LSTM + Bahdanau model
python main.py --config configs/lstm_bahdanau.yaml

# Train LSTM + Luong model
python main.py --config configs/lstm_luong.yaml
```

#### Command-Line Arguments

Override configuration parameters via command-line arguments:

```bash
# Override batch size and number of epochs
python main.py --config configs/transformer.yaml \
    --batch_size 32 \
    --num_epochs 20

# Override processing level (word or phoneme)
python main.py --config configs/transformer.yaml \
    --level word

# Override random seed
python main.py --config configs/transformer.yaml \
    --seed 42

# Resume training from checkpoint
python main.py --config configs/transformer.yaml \
    --resume checkpoints/transformer/model_epoch_001.pt

# Combine multiple overrides
python main.py --config configs/transformer.yaml \
    --batch_size 16 \
    --num_epochs 10 \
    --level phoneme \
    --seed 123
```

#### Available Arguments

| Argument | Type | Description | Default |
|----------|------|-------------|---------|
| `--config` | str | Path to YAML configuration file | `configs/transformer.yaml` |
| `--batch_size` | int | Batch size (overrides config) | None |
| `--num_epochs` | int | Number of training epochs (overrides config) | None |
| `--level` | str | Processing level: `word` or `phoneme` (overrides config) | None |
| `--seed` | int | Random seed for reproducibility (overrides config) | None |
| `--resume` | str | Path to checkpoint file to resume training | None |


## Configuration

### Configuration File Structure

Each YAML configuration file follows this structure:

```yaml
model:
  name: "transformer"              # Model name: "transformer", "lstm_bahdanau", or "lstm_luong"
  embed_dim: 512                   # Embedding dimension
  hidden_dim: 512                  # Hidden dimension (for LSTM)
  num_layers: 6                    # Number of layers
  dropout: 0.1                     # Dropout rate
  num_heads: 8                     # Number of attention heads (Transformer only)
  ff_dim: 2048                     # Feed-forward dimension (Transformer only)
  attention_type: "general"        # Attention type (LSTM Luong only: "general", "dot", "concat")

training:
  batch_size: 8                    # Batch size
  num_epochs: 10                   # Number of epochs
  learning_rate: 0.001             # Learning rate
  optimizer: "adamw"               # Optimizer: "adam", "sgd", or "adamw"
  scheduler: "cosine"              # Learning rate scheduler
  clip_grad_norm: 5.0              # Gradient clipping threshold
  warmup_steps: 4000               # Warmup steps (Transformer only)
  eval_every: 1000                 # Evaluation frequency (steps)
  save_every: 5000                 # Checkpoint saving frequency (steps)

data:
  # Special token IDs
  sos_id: 1                        # Start-of-sequence token ID
  eos_id: 2                        # End-of-sequence token ID
  pad_id: 0                        # Padding token ID
  unk_id: 3                        # Unknown token ID
  
  # Processing level (must match for source and target)
  source_level: "word"              # Source level: "word" or "phoneme"
  target_level: "word"              # Target level: "word" or "phoneme" (must match source_level)
  
  # Data file paths
  train_src: "path/to/train.en"
  train_tgt: "path/to/train.vi"
  dev_src: "path/to/dev.en"
  dev_tgt: "path/to/dev.vi"
  test_src: "path/to/test.en"
  test_tgt: "path/to/test.vi"
  
  # Vocabulary settings
  vocab_json_train: "path/to/vocab.json"  # For phoneme-level processing
  min_count: 3                     # Minimum word count for vocabulary
  max_seq_len: 64                  # Maximum sequence length

device: "cuda"                     # Device: "cuda" or "cpu"
seed: 42                           # Random seed
```

### Using Configuration in Code

```python
from configs.config import Config

# Load configuration from YAML file
config = Config.from_yaml('configs/transformer.yaml')

# Access configuration values using dot notation
print(config.model.embed_dim)
print(config.training.batch_size)
print(config.data.source_level)

## Evaluation Metrics

All models are evaluated using standard NMT metrics at both **word-level** and **phoneme-level**:

- **BLEU**: BLEU@1, BLEU@2, BLEU@3, BLEU@4
- **ROUGE**: ROUGE-L (Longest Common Subsequence)
- **METEOR**: METEOR score

Metrics are computed automatically during validation and can be logged for analysis.

## Processing Levels

The project supports two processing levels:

- **Word-level**: Translation at word granularity (word â†’ word)
- **Phoneme-level**: Translation at phoneme granularity (phoneme â†’ phoneme)

**Important Note:** `source_level` and `target_level` must match. The project does not support mixed-level translation (e.g., word â†’ phoneme or phoneme â†’ word). Both source and target must use the same level.

## ğŸ“š References

1. **Transformer**: Vaswani, A., et al. (2017). "Attention is All You Need". *Advances in Neural Information Processing Systems*, 30.

2. **Bahdanau Attention**: Bahdanau, D., Cho, K., & Bengio, Y. (2015). "Neural Machine Translation by Jointly Learning to Align and Translate". *International Conference on Learning Representations*.

3. **Luong Attention**: Luong, M. T., Pham, H., & Manning, C. D. (2015). "Effective Approaches to Attention-based Neural Machine Translation". *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*.

## ğŸ“„ License

[Add license information here]

## ğŸ‘¥ Contributors

[Add contributor information here]

## ğŸ™ Acknowledgments

This project is part of research on English-Vietnamese Neural Machine Translation, exploring both word-level and phoneme-level approaches to translation.
